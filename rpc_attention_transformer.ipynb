{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFbHYtX4NuU5aN+OdxPOCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elrrowwe/robust-principal-component-attention/blob/main/rpc_attention_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains my experiments with integrating robust principal component attention (RPCA) introduced in [here](https://arxiv.org/pdf/2406.13762) with the Karpathy from-scratch implementation of transformers."
      ],
      "metadata": {
        "id": "_mBvqQ-RpHtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eyA-FHQxXCQO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining some hyperparameters\n",
        "batch_size = 16  # how many sequences (sentences of some length) are processed at once\n",
        "block_size = 32  # how long the aforementioned sequences are (context length)\n",
        "train_iters = 2000\n",
        "eval_interval = 100\n",
        "lr = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embed = 64  # the number of embedding dimensions (the dimensionality of embedding vectors)\n",
        "n_head = 4  # the number of heads in the multi-head attention layer\n",
        "n_layer = 6\n",
        "dropout = 0.0  # the dropout rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpUgBrHepagy",
        "outputId": "950d9b1d-9aa8-43e9-a3a0-c7eccc32a113"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oliver_twist = open('pg730.txt', 'r', encoding='utf8').read()"
      ],
      "metadata": {
        "id": "ae1RPC19qL_I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the number of characters (tokens) in the dataset\n",
        "print(len(list(oliver_twist)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf2TIYQ-qPit",
        "outputId": "ca902252-72c3-45ec-975f-f714bd2b7292"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1079372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting all the characters, like in the n-gram model\n",
        "chars = sorted(list(set(oliver_twist)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "Xa4HgXInqEEK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple (de-)tokenizer\n",
        "stoi = {s: i for i,s in enumerate(chars)}\n",
        "itos = {i: s for i,s in enumerate(chars)}"
      ],
      "metadata": {
        "id": "ykx5gfX5qF4I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[ch] for ch in s]  # tokenize some characters\n",
        "decode = lambda i: ''.join([itos[num] for num in i])  # detokenize some integers"
      ],
      "metadata": {
        "id": "b7TSKksZqHkD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing the entire data set\n",
        "enc = torch.tensor(encode(oliver_twist), dtype=torch.long, device=device)"
      ],
      "metadata": {
        "id": "aY-kQ4lzqJXR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the text into train, test portions\n",
        "train, test = train_test_split(enc, shuffle=False, test_size=0.1)"
      ],
      "metadata": {
        "id": "e95SCeArqLQi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Get a batch of training examples (sentences) from the train/test dataset.\n",
        "    \"\"\"\n",
        "    data = train if split == 'train' else test\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "Ho441Idtqe8Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    Estimate the average loss of the model on the train/test dataset.\n",
        "    \"\"\"\n",
        "    out = {}  # the output placeholder dictionary\n",
        "    bi.eval()\n",
        "    for split in ['train', 'test']:\n",
        "        losses = torch.zeros(eval_iters, device=device)\n",
        "\n",
        "        for i in range(eval_iters):\n",
        "            X, y = get_batch(split)\n",
        "            logits, loss = bi(X, y)\n",
        "            losses[i] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "    bi.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "gOfd0VwDqn2J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention implementation from [here](https://github.com/rachtsy/KPCA_code/blob/master/Robust/softmax.py)"
      ],
      "metadata": {
        "id": "WtWDD4ji9jL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: add explanatory comments\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.,\n",
        "                 robust=False, layerth=0, n=1, lambd=0, layer=0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.n = n\n",
        "        self.lambd = lambd\n",
        "        self.layer = layer\n",
        "        # sqrt (D)\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.layerth = layerth\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.robust = robust\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        # q,k -> B -> heads -> n -> features\n",
        "        qkv = self.qkv(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        # robust principal components attention\n",
        "        if (self.robust and self.layer < 0) or (self.robust and self.layerth==self.layer):\n",
        "            l = torch.zeros((B,self.num_heads,N,C // self.num_heads)).to(torch.device(\"cuda\"), non_blocking=True)\n",
        "            y = torch.zeros((B,self.num_heads,N,C // self.num_heads)).to(torch.device(\"cuda\"), non_blocking=True)\n",
        "\n",
        "            mu=N*C/4/k.norm(p=1,dim=[-1,-2],keepdim=True)\n",
        "\n",
        "            for i in range(0,self.n-1):\n",
        "                s = k-l+y/mu\n",
        "                s_less = s.le(-self.lambd*mu).int()\n",
        "                s_more = s.ge(self.lambd*mu).int()\n",
        "                s = (s-self.lambd*mu)*s_more + (s+self.lambd*mu)*s_less\n",
        "                k2 = k-s-y/mu\n",
        "                l = (k2 @ k2.transpose(-2, -1)) * self.scale\n",
        "                l = l.softmax(dim=-1)\n",
        "                l = l @ v\n",
        "                y = y+mu*(k-l-s)\n",
        "\n",
        "            s = k-l+y/mu\n",
        "            s_less = s.le(-self.lambd*mu).int()\n",
        "            s_more = s.ge(self.lambd*mu).int()\n",
        "            s = (s-self.lambd*mu)*s_more + (s+self.lambd*mu)*s_less\n",
        "            k2 = k-s-y/mu\n",
        "            l = (k2 @ k2.transpose(-2, -1)) * self.scale\n",
        "            l = l.softmax(dim=-1)\n",
        "            l = self.attn_drop(l)\n",
        "            x = l @ v\n",
        "            y = y+mu*(k-x-s)\n",
        "\n",
        "        # symmetric attention\n",
        "        else:\n",
        "            attn = (k @ k.transpose(-2, -1)) * self.scale\n",
        "            attn = attn.softmax(dim=-1)\n",
        "\n",
        "            attn = self.attn_drop(attn)\n",
        "\n",
        "            x = (attn @ v)\n",
        "\n",
        "        x = x.transpose(1, 2).reshape(B,N,C)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "joxkRlCGrKzF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed).to(device),\n",
        "            nn.ReLU().to(device),\n",
        "            nn.Linear(4 * n_embed, n_embed).to(device),\n",
        "            nn.Dropout(dropout).to(device)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "WF28LtQI_nJj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
        "                 dropout=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, layerth=None,\n",
        "                 robust=False, n=1, lambd=0, layer=0):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "                                    attn_drop=attn_drop, proj_drop=drop, robust=robust,\n",
        "                                    layerth=layerth, n=n, lambd=lambd, layer=layer)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Dense(n_embed=n_embed)\n",
        "        self.layerth = layerth\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.norm2(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "calhCovH-sfq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_dim=n_embed,\n",
        "                 num_heads=n_head,\n",
        "                 qkv_bias=False,\n",
        "                 dropout=dropout,\n",
        "                 attn_drop_rate=dropout,\n",
        "                 robust=True):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed).to(device)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed).to(device)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size).to(device)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
        "                attn_drop=attn_drop_rate, layerth = i, robust=robust)\n",
        "            for i in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed).to(device)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_embeddings = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_embeddings + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "fbOtxZNv_NyT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi = BigramModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(bi.parameters(), lr=lr)\n",
        "\n",
        "for i in range(train_iters):\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f'step: {i}, train_loss: {losses[\"train\"]}, test loss: {losses[\"test\"]}')\n",
        "\n",
        "    xt, yt = get_batch('train')\n",
        "    logits, loss = bi(xt, yt)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(bi.generate(idx, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_Z5OockBfXr",
        "outputId": "61ea13d3-8c41-4080-e27d-25c701ab1dea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train_loss: 4.561573028564453, test loss: 4.563760280609131\n",
            "step: 100, train_loss: 2.6156508922576904, test loss: 2.607220411300659\n",
            "step: 200, train_loss: 2.2952284812927246, test loss: 2.311758279800415\n",
            "step: 300, train_loss: 0.36925262212753296, test loss: 0.38749638199806213\n",
            "step: 400, train_loss: 0.18664829432964325, test loss: 0.2270771712064743\n",
            "step: 500, train_loss: 0.13469599187374115, test loss: 0.18306760489940643\n",
            "step: 600, train_loss: 0.11643478274345398, test loss: 0.14675810933113098\n",
            "step: 700, train_loss: 0.11148347705602646, test loss: 0.13251574337482452\n",
            "step: 800, train_loss: 0.09771157056093216, test loss: 0.11405660957098007\n",
            "step: 900, train_loss: 0.09336519986391068, test loss: 0.1087425947189331\n",
            "step: 1000, train_loss: 0.09329553693532944, test loss: 0.10358624905347824\n",
            "step: 1100, train_loss: 0.09017336368560791, test loss: 0.09857236593961716\n",
            "step: 1200, train_loss: 0.08745908737182617, test loss: 0.09451554715633392\n",
            "step: 1300, train_loss: 0.08660595118999481, test loss: 0.09337371587753296\n",
            "step: 1400, train_loss: 0.08433369547128677, test loss: 0.08674892038106918\n",
            "step: 1500, train_loss: 0.08272276818752289, test loss: 0.08698771893978119\n",
            "step: 1600, train_loss: 0.08012748509645462, test loss: 0.08630062639713287\n",
            "step: 1700, train_loss: 0.08115977048873901, test loss: 0.08328734338283539\n",
            "step: 1800, train_loss: 0.07844986766576767, test loss: 0.08321429789066315\n",
            "step: 1900, train_loss: 0.07682516425848007, test loss: 0.07844836264848709\n",
            "\n",
            "LLHHHHHHHHEREEEEERAAAAAAANOOOOOaQ\n",
            "PIWO\"Dn 'Wottthe to'Whe'\n",
            "'Yono, cot the thered.'\n",
            "\n",
            "'P'en mow, whe whas hor the whor the werEe then on viroug.\n",
            "Th, ind wo\n",
            "theol, derpo\n",
            "seden wor. We! bures. Wes Nne raverelll, beur walded alle ghas ster'the, Mrydenen hes in tofing med  he coowely all ber lapring brsefutis the cauty hot ar! Rhelr corterte.'\n",
            " hynor her\n",
            "to yore crily\n",
            "cin werto o wrling and Iedd heres\n",
            "tote romene the hor he oumom Ice cornno, wheer Wo he ber\n",
            "this mas aked wald mehe\n",
            "medy he wadund thes whim wher stewer, yoro Word no bene\n",
            "roner acin'\n",
            "\n",
            "\n",
            "Thase ronodery br meternindely be nhaverlde  betletter surly.'\n",
            "\n",
            "'I'Wouc..\n",
            "\n",
            "'Wist he him the.  thers tor to, whre bur toerd\n",
            "and afusntin the wors. I_peined her, Yf?\n",
            "her unly aruothedo the acther jo\n",
            "wid. Ter, Cen, arn the\n",
            "kine car as thimt uptrotmall,\n",
            "deld, at bep\n",
            "ouen kitthe peoting sons wo whim vothe wald am led beld. Darto war, trourllllg, fot wo; andeycled\n",
            "vinomdesitt_leldoughermyy, che\n",
            "windyted he\n",
            "goornint ald Goo co rod worse.\n",
            "\n",
            "'Wher.' Hroter, were yonor, oly jhetun rorrep, thee the do\n",
            "theedor, lp oo\n",
            "he matd, Iores, ow hre Horted-e--ulve'\n",
            "\n",
            "'We thorto ser, anardined 'Naturive, prod that wart int rego sto lotwer.\n",
            "\n",
            "'Where, fored the  Nha herse Fhom a berpit ththnedWesuning to the tiw wisto ro ther'tr.\n",
            "'\n",
            "\n",
            "'Whexthe, Mrorlat whothy,'â€™ied no tone herer, he\n",
            "son whe, pred ar athee, dint ariver yo,' herc\n",
            "ave intth hare som to fro're and themor his Tininggl.\n",
            "\n",
            "'wiskeint'e\n",
            "Mre?' Oalr bo the mano there the\n",
            "werten, re mernin that in wI metxreter he forelmy wor the Mote ale bokins, we' Cader hinith thilere the wor thrrstel thorser wing prory ofth ackeer mrineder.'\n",
            "'his plo'ked, in thew rrede word Jo premir twe forernor, fotheror of therepevellirneg ro!-; ofcroucer forr beve, prredild he ard oun, hie lellllseredLerne. \n",
            ". IWotore whe merydom, e\n",
            ".PWo Oron her ben\n",
            "ter, Ao Worghererr Yrm ern. ' thayow rotheresan\n",
            "oklll ton rin then sretris previr\n",
            "matte bed, I drid!', yo wher ofwering\n",
            "od witle'the\n",
            "rille, yonorecmaMrser and ho, rred ar ow wh thon\n"
          ]
        }
      ]
    }
  ]
}
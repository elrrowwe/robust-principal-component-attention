{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains my experiments with integrating robust principal component attention (RPCA) introduced in [here](https://arxiv.org/pdf/2406.13762) with the Karpathy from-scratch implementation of transformers.","metadata":{"id":"_mBvqQ-RpHtX"}},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ipD5J30OzsH","outputId":"38580c58-5994-4937-bba4-d2189063ae80","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:05:54.446442Z","iopub.execute_input":"2025-01-14T19:05:54.446755Z","iopub.status.idle":"2025-01-14T19:05:58.528821Z","shell.execute_reply.started":"2025-01-14T19:05:54.446721Z","shell.execute_reply":"2025-01-14T19:05:58.527941Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom sklearn.model_selection import train_test_split\nimport tiktoken\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"id":"eyA-FHQxXCQO","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:05:58.530043Z","iopub.execute_input":"2025-01-14T19:05:58.530323Z","iopub.status.idle":"2025-01-14T19:06:01.716097Z","shell.execute_reply.started":"2025-01-14T19:05:58.530302Z","shell.execute_reply":"2025-01-14T19:06:01.715173Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# defining some hyperparameters\nbatch_size = 16  # how many sequences (sentences of some length) are processed at once\nblock_size = 64  # how long the aforementioned sequences are (context length)\ntrain_iters = 3000\neval_interval = 100\nlr = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\neval_iters = 200\nn_embed = 128   # the number of embedding dimensions (the dimensionality of embedding vectors)\nn_head = 4  # the number of heads in the multi-head attention layer\nn_layer = 6\ndropout = 0.2  # the dropout rate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpUgBrHepagy","outputId":"9ded9347-65d6-4601-a8f9-14a1823715e6","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:16:55.048416Z","iopub.execute_input":"2025-01-14T19:16:55.048820Z","iopub.status.idle":"2025-01-14T19:16:55.054438Z","shell.execute_reply.started":"2025-01-14T19:16:55.048754Z","shell.execute_reply":"2025-01-14T19:16:55.053568Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# tiny stories train and validation splits","metadata":{"id":"ouNKAUNPPrhM"}},{"cell_type":"code","source":"ds = load_dataset(\"roneneldan/TinyStories\", split='train')\nds.to_list()\nds = ['<|story_start|> ' + text['text'] + ' <|story_end|>' for text in ds]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ej5Wt5VGIWr6","outputId":"2f75933f-a6a6-4b23-b892-ee253ed7f60a","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:06:01.743335Z","iopub.execute_input":"2025-01-14T19:06:01.743575Z","iopub.status.idle":"2025-01-14T19:06:46.174444Z","shell.execute_reply.started":"2025-01-14T19:06:01.743555Z","shell.execute_reply":"2025-01-14T19:06:46.173650Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"ds_val = load_dataset(\"roneneldan/TinyStories\", split='validation')\nds_val.to_list()\nds_val = ['<|story_start|> ' + text['text'] + ' <|story_end|>' for text in ds_val]","metadata":{"id":"02LZ2-x6Oyw0","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:06:46.177322Z","iopub.execute_input":"2025-01-14T19:06:46.177578Z","iopub.status.idle":"2025-01-14T19:06:48.868918Z","shell.execute_reply.started":"2025-01-14T19:06:46.177558Z","shell.execute_reply":"2025-01-14T19:06:48.868261Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# enc = tiktoken.encoding_for_model(\"gpt-4o\")\n# enc.n_vocab\n\ncl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n\n# appending the special \"<|story_start|>\", \"<|story_end|>\" to the vocab of the\nenc = tiktoken.Encoding(\n    name=\"cl100k_im\",\n    pat_str=cl100k_base._pat_str,\n    mergeable_ranks=cl100k_base._mergeable_ranks,\n    special_tokens={\n        **cl100k_base._special_tokens,\n        \"<|story_start|>\": 100264,\n        \"<|story_end|>\": 100265,\n    }\n)","metadata":{"id":"TuHP32bKPPU8","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:06:48.870382Z","iopub.execute_input":"2025-01-14T19:06:48.870653Z","iopub.status.idle":"2025-01-14T19:06:49.258755Z","shell.execute_reply.started":"2025-01-14T19:06:48.870632Z","shell.execute_reply":"2025-01-14T19:06:49.257855Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"vocab_size = enc.n_vocab","metadata":{"id":"Xa4HgXInqEEK","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:06:49.259703Z","iopub.execute_input":"2025-01-14T19:06:49.259974Z","iopub.status.idle":"2025-01-14T19:06:49.263617Z","shell.execute_reply.started":"2025-01-14T19:06:49.259952Z","shell.execute_reply":"2025-01-14T19:06:49.262830Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# tokenizing the entire data set\n\nallowed_special = [\"<|story_start|>\", \"<|story_end|>\"]\n\nencoded_tiny_stories_train = torch.tensor(enc.encode(' '.join(ds), allowed_special=set(allowed_special)), dtype=torch.long, device=device)","metadata":{"id":"aY-kQ4lzqJXR","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:06:49.264441Z","iopub.execute_input":"2025-01-14T19:06:49.264665Z","iopub.status.idle":"2025-01-14T19:11:09.576379Z","shell.execute_reply.started":"2025-01-14T19:06:49.264636Z","shell.execute_reply":"2025-01-14T19:11:09.575631Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"len(encoded_tiny_stories_train)","metadata":{"id":"k7-CbK-vHqdq","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:20.997565Z","iopub.execute_input":"2025-01-14T19:11:20.997916Z","iopub.status.idle":"2025-01-14T19:11:21.004822Z","shell.execute_reply.started":"2025-01-14T19:11:20.997888Z","shell.execute_reply":"2025-01-14T19:11:21.004051Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"459405623"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# splitting the text into train, test portions\ntrain, test = train_test_split(encoded_tiny_stories_train, shuffle=False, test_size=0.1)","metadata":{"id":"e95SCeArqLQi","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:28.896262Z","iopub.execute_input":"2025-01-14T19:11:28.896604Z","iopub.status.idle":"2025-01-14T19:11:30.606726Z","shell.execute_reply.started":"2025-01-14T19:11:28.896575Z","shell.execute_reply":"2025-01-14T19:11:30.605619Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_batch(split):\n    \"\"\"\n    Get a batch of training examples (sentences) from the train/test dataset.\n    \"\"\"\n    data = train if split == 'train' else test\n    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"id":"Ho441Idtqe8Z","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:31.812582Z","iopub.execute_input":"2025-01-14T19:11:31.812946Z","iopub.status.idle":"2025-01-14T19:11:31.818334Z","shell.execute_reply.started":"2025-01-14T19:11:31.812916Z","shell.execute_reply":"2025-01-14T19:11:31.817399Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    \"\"\"\n    Estimate the average loss of the model on the train/test dataset.\n    \"\"\"\n    out = {}  # the output placeholder dictionary\n    bi.eval()\n    for split in ['train', 'test']:\n        losses = torch.zeros(eval_iters, device=device)\n\n        for i in range(eval_iters):\n            X, y = get_batch(split)\n            logits, loss = bi(X, y)\n            losses[i] = loss.item()\n\n        out[split] = losses.mean()\n    bi.train()\n    return out","metadata":{"id":"gOfd0VwDqn2J","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:33.249103Z","iopub.execute_input":"2025-01-14T19:11:33.249409Z","iopub.status.idle":"2025-01-14T19:11:33.254450Z","shell.execute_reply.started":"2025-01-14T19:11:33.249386Z","shell.execute_reply":"2025-01-14T19:11:33.253436Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Attention implementation from [here](https://github.com/rachtsy/KPCA_code/blob/master/Robust/softmax.py)","metadata":{"id":"WtWDD4ji9jL6"}},{"cell_type":"code","source":"# TODO: add explanatory comments\n\nclass Attention(nn.Module):\n    \"\"\"\n\n    \"\"\"\n    def __init__(self, dim=n_embed, num_heads=n_head, qkv_bias=False, attn_drop=dropout, proj_drop=dropout,\n                 robust=False, layerth=0, n=1, lambd=0, layer=0):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.n = n\n        self.lambd = lambd\n        self.layer = layer\n        # sqrt (D)\n        self.scale = head_dim ** -0.5\n        self.layerth = layerth\n\n        self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.robust = robust\n\n    def forward(self, x):\n        B, N, C = x.shape\n        # q,k -> B -> heads -> n -> features\n        qkv = self.qkv(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n\n        # robust principal components attention\n        if (self.robust and self.layer < 0) or (self.robust and self.layerth==self.layer):\n            l = torch.zeros((B,self.num_heads,N,C // self.num_heads)).to(torch.device(\"cuda\"), non_blocking=True)\n            y = torch.zeros((B,self.num_heads,N,C // self.num_heads)).to(torch.device(\"cuda\"), non_blocking=True)\n\n            mu=N*C/4/k.norm(p=1,dim=[-1,-2],keepdim=True)\n\n            # the Principal Attention Pursuit algorithm\n            for i in range(0,self.n-1):\n                s = k-l+y/mu\n                s_less = s.le(-self.lambd*mu).int()\n                s_more = s.ge(self.lambd*mu).int()\n                s = (s-self.lambd*mu)*s_more + (s+self.lambd*mu)*s_less\n                k2 = k-s-y/mu\n                l = (k2 @ k2.transpose(-2, -1)) * self.scale\n                l = l.softmax(dim=-1)\n                l = l @ v\n                y = y+mu*(k-l-s)\n\n            s = k-l+y/mu\n            s_less = s.le(-self.lambd*mu).int()\n            s_more = s.ge(self.lambd*mu).int()\n            s = (s-self.lambd*mu)*s_more + (s+self.lambd*mu)*s_less\n            k2 = k-s-y/mu\n            l = (k2 @ k2.transpose(-2, -1)) * self.scale\n            l = l.softmax(dim=-1)\n            l = self.attn_drop(l)\n            x = l @ v\n            y = y+mu*(k-x-s)\n\n        # symmetric attention\n        else:\n            attn = (k @ k.transpose(-2, -1)) * self.scale\n            attn = attn.softmax(dim=-1)\n\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v)\n\n        x = x.transpose(1, 2).reshape(B,N,C)\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x","metadata":{"id":"joxkRlCGrKzF","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:35.664756Z","iopub.execute_input":"2025-01-14T19:11:35.665107Z","iopub.status.idle":"2025-01-14T19:11:35.677007Z","shell.execute_reply.started":"2025-01-14T19:11:35.665083Z","shell.execute_reply":"2025-01-14T19:11:35.676052Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Dense(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4 * n_embed).to(device),\n            nn.ReLU().to(device),\n            nn.Linear(4 * n_embed, n_embed).to(device),\n            nn.Dropout(dropout).to(device)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"id":"WF28LtQI_nJj","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:37.325334Z","iopub.execute_input":"2025-01-14T19:11:37.325638Z","iopub.status.idle":"2025-01-14T19:11:37.330744Z","shell.execute_reply.started":"2025-01-14T19:11:37.325615Z","shell.execute_reply":"2025-01-14T19:11:37.329813Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\"\n\n    \"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 dropout=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, layerth=None,\n                 robust=False, n=1, lambd=0, layer=0):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                                    attn_drop=attn_drop, proj_drop=drop, robust=robust,\n                                    layerth=layerth, n=n, lambd=lambd, layer=layer)\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Dense(n_embed=n_embed)\n        self.layerth = layerth\n\n    def forward(self, x):\n        x = x + self.dropout(self.attn(self.norm1(x)))\n        x = x + self.dropout(self.mlp(self.norm2(x)))\n        return x","metadata":{"id":"calhCovH-sfq","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:11:39.211256Z","iopub.execute_input":"2025-01-14T19:11:39.211567Z","iopub.status.idle":"2025-01-14T19:11:39.217492Z","shell.execute_reply.started":"2025-01-14T19:11:39.211544Z","shell.execute_reply":"2025-01-14T19:11:39.216684Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class BigramModel(nn.Module):\n    def __init__(self,\n                 embed_dim=n_embed,\n                 num_heads=n_head,\n                 qkv_bias=False,\n                 dropout=dropout,\n                 attn_drop_rate=dropout,\n                 robust=True):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed).to(device)\n        self.position_embedding_table = nn.Embedding(block_size, n_embed).to(device)\n        self.lm_head = nn.Linear(n_embed, vocab_size).to(device)\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                attn_drop=attn_drop_rate, layerth = i, robust=robust)\n            for i in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embed).to(device)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_embeddings = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_embeddings + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx","metadata":{"id":"fbOtxZNv_NyT","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:17:05.118162Z","iopub.execute_input":"2025-01-14T19:17:05.118537Z","iopub.status.idle":"2025-01-14T19:17:05.127184Z","shell.execute_reply.started":"2025-01-14T19:17:05.118507Z","shell.execute_reply":"2025-01-14T19:17:05.126266Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp","metadata":{"id":"-uT7_j2YHPg-","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:17:07.399944Z","iopub.execute_input":"2025-01-14T19:17:07.400356Z","iopub.status.idle":"2025-01-14T19:17:07.404684Z","shell.execute_reply.started":"2025-01-14T19:17:07.400316Z","shell.execute_reply":"2025-01-14T19:17:07.403844Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"bi = BigramModel().to(device)\n\nprint(f'the number of trainable parameters in the model: {get_n_params(bi)}')\n\noptimizer = torch.optim.AdamW(bi.parameters(), lr=lr)\n\nfor i in range(train_iters):\n    if i % eval_interval == 0:\n        losses = estimate_loss()\n        print(f'step: {i}, train_loss: {losses[\"train\"]}, test loss: {losses[\"test\"]}')\n\n    xt, yt = get_batch('train')\n    logits, loss = bi(xt, yt)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"id":"j_Z5OockBfXr","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:17:08.899576Z","iopub.execute_input":"2025-01-14T19:17:08.899910Z","iopub.status.idle":"2025-01-14T19:21:50.550022Z","shell.execute_reply.started":"2025-01-14T19:17:08.899884Z","shell.execute_reply":"2025-01-14T19:21:50.549260Z"}},"outputs":[{"name":"stdout","text":"the number of trainable parameters in the model: 26868661\nstep: 0, train_loss: 11.698225021362305, test loss: 11.699721336364746\nstep: 100, train_loss: 5.583298206329346, test loss: 5.546853065490723\nstep: 200, train_loss: 4.796993732452393, test loss: 4.769114971160889\nstep: 300, train_loss: 4.4156317710876465, test loss: 4.421930313110352\nstep: 400, train_loss: 4.220678329467773, test loss: 4.211587429046631\nstep: 500, train_loss: 4.002357006072998, test loss: 3.9903810024261475\nstep: 600, train_loss: 3.583458185195923, test loss: 3.578867197036743\nstep: 700, train_loss: 3.046708822250366, test loss: 3.041980266571045\nstep: 800, train_loss: 2.6715424060821533, test loss: 2.652944803237915\nstep: 900, train_loss: 2.318943738937378, test loss: 2.322850465774536\nstep: 1000, train_loss: 2.007035493850708, test loss: 2.0009891986846924\nstep: 1100, train_loss: 1.68059241771698, test loss: 1.6459548473358154\nstep: 1200, train_loss: 1.326859712600708, test loss: 1.3224543333053589\nstep: 1300, train_loss: 1.0010969638824463, test loss: 1.000279188156128\nstep: 1400, train_loss: 0.7789131999015808, test loss: 0.7749552726745605\nstep: 1500, train_loss: 0.609312891960144, test loss: 0.6162387132644653\nstep: 1600, train_loss: 0.48832693696022034, test loss: 0.48427248001098633\nstep: 1700, train_loss: 0.41116878390312195, test loss: 0.3928603231906891\nstep: 1800, train_loss: 0.3317554295063019, test loss: 0.3284052312374115\nstep: 1900, train_loss: 0.29622969031333923, test loss: 0.28433623909950256\nstep: 2000, train_loss: 0.2678266167640686, test loss: 0.2591458857059479\nstep: 2100, train_loss: 0.23614661395549774, test loss: 0.23528480529785156\nstep: 2200, train_loss: 0.2068934589624405, test loss: 0.20929133892059326\nstep: 2300, train_loss: 0.18643464148044586, test loss: 0.1859244555234909\nstep: 2400, train_loss: 0.18623992800712585, test loss: 0.1746620088815689\nstep: 2500, train_loss: 0.17299975454807281, test loss: 0.172300785779953\nstep: 2600, train_loss: 0.16071376204490662, test loss: 0.16642184555530548\nstep: 2700, train_loss: 0.15557070076465607, test loss: 0.1557285189628601\nstep: 2800, train_loss: 0.14950847625732422, test loss: 0.146528959274292\nstep: 2900, train_loss: 0.14630690217018127, test loss: 0.1450795978307724\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"idx = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(enc.decode(bi.generate(idx, max_new_tokens=2000)[0].tolist()))","metadata":{"id":"h-jkSAL4vUWh","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T19:22:14.895669Z","iopub.execute_input":"2025-01-14T19:22:14.896070Z","iopub.status.idle":"2025-01-14T19:22:25.965912Z","shell.execute_reply.started":"2025-01-14T19:22:14.896038Z","shell.execute_reply":"2025-01-14T19:22:25.964993Z"}},"outputs":[{"name":"stdout","text":"! my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my might had to see it got back and said through tea is very had just good.\"\n\n\"Of diamond others. I says he saw her jets. You have toast and Ben said.\n\n\"Good for a pirateaying it was an to\"Stop, and ran to please to be. <|story_end|> <|story_end|> <|story_start|> Once there, Jack was not have, but never just moved, and Ben had a great place?\" when she heard in the road. I wanted to play with with with the bird was very nice, she saw it. She's go to go. it him.\"\n\n realized with read Ben was a boxTom was very hot and right. I am with her, white pile out. never want up.\n\nLily. It found it was like with it with it needed the number. There. <|story_start|> Tom was able to it in the adventure. It on he noticed out and wish.\n\nThe man started to play with the food and put the mom and powerful with his nap and around around and it would be forgive. Kim. Heaked. But a dead she went shop. He bigters. You would ask, but a sand to catch. They happy and see the leaves.\n\nJ, she said, said, but you. Mom. Amy said, I am welcome and said. He said, \"No, they had to go and could't see to go.\n\n\"Okay.\n\n\"No and said, I.\n\n\"That's Ben. Lily was I was very she reminded of birds. Sam had.\n\nAt,\" Lily went that the ladder. It was worked.\"\n\nJamie. It was very afraid. One day. Lily's justMmm with his toys, that found it in fun fun on fun.\n\nLily! She liked to pack to show trust show to care they on on on a guitar.\n\nLily asked ask to play.\"\n\nHis mom, Timmy had on the unknown with very very his very fun and went to play the ground and said a news and destroy, there, Ben feel using it was fun. She with the corner there? It is a grass time there was not nice.\n\n\"The.\n\nBen was loyal branch. It did owned saw something happened and opened with the cat.\n\nFirst.\n\nOne day!\" said at the toy toy at the cookie with the blue with.\n\nBen says to push to be her new place. Luckily. She wanted to have little were many peaceful. He had flying some up and just and he likes to your paw when when, her park. it was scary and see tail and houses. She was not found and show it noticed. Spoto and Dad as color, Lily heard her birthday, I thanked him through the scissors a tedâ€™ sighed, \"Mommy. It looks until she made a closer was stealing.\n\n\"Look, she nose.â€™? He likes.\n\nThey had in theirShe could see it and they had along, he was not their keep his lesson together on a time, she. She was so happy was so curious in what they found a beautiful silver and he was not toys in the forest that day, she likes to get to join. Now't find your favorite picture. It flew well. It had. Be long and saw not am not know something loud, but saw something bigger. From, in it in other in around back in a black just find his favorite store it. <|story_start|> Toby was very he was a boy named <|story_start|> most deaf night and was so fierce stick, two and dirty and she was a little girl asked the rat. She was the best friend to have her feel too.\n\nThe little boy, she walked flew to clean the ornament and says, and found his knees to show to show to show to, the same the train. He has, wait at Lily see the rain and went. Lily sat Hi food with her toy toy were nice toys and play on the ruler.\n\nSo, her so happy with his toys in the joy back home go and sorry and sorry the music and never like good coin them together and in the sea and saw a little boy, he found a secret? he was the door. They thought through they tried and pieces and each and he. He was bright was I scared, she learnt, the rock.\n\n\"Let a car. It when you see the horizon the conflict. She was a the truck andlige played with with every the tough want to listen to cannot to sew eating gloves with one happened and she needed with her cheat and. She says that to eat. They had not have. car picked near the car. Ituzz. She saw she saw outside, I win new plastic of a time Lily thought to go with the calendar. She would other hair, she waved. ItWhat, Lily. He ran wait to just do. <|story_end|> <|story_start|> Once there bird. She was asking a lady. She has the cloth you with the people kept both shared on her books. He ran to come and cool on a window and she did things run to play with her garden. She got Once to play, that she took him to share you boy. Don't buy with her cat was not andâ€ Tom with the cat.\n\nThe rabbit.\nâ€ were two and Jack. I felt all played by you. You should feel beautiful things more knights Anna was best flea and grateful andLily he€ she decided them on. You are very proud and went to it with one, b looks to scream. It your shoes. She years fireplace. You walk. Lucy agreed. She about her back and sky and read the room.\n\nLily missed their rain and rot folded adventure, crunch!\" she looked Max saw a time, Tim the crane had fun on the cookie. She was less playing with her dad was very very good. This old about her diamond with her window together. Jack was a was a cake music, Ben went home in in his club. It was so coin that it something day - she liked to nap to show alone in the dog. Max was a little bunny and she managed about and in his costume. She went on on the rules.\n\nThe first she flew and likes it loved to have a stack. She waved to she took a soft lady and started was so she got and had a old of are tells the thermometer the so very trying day on last last lizard with new mug. You are so happy things as the friend when the steam he helped the car and said doing. You won am not have two and saw her shot with the town and saw a break to see under her son. The small was the piano and she accidentally. She asked his paw on on its ball. It moved of drawings, but they returned in a vasekeeper. He was being not see the empty on the van. <|story_end|> <|story_start|> Once upon a time, he found a walk. She ran to feel back colors and bigger. Joe was a brick and liked to show with Lily every day named Lily was too.\n\nOne day, there was there are it, next day with One day.\"\n\nLily. I never had played with them. She had will hit to see long. You went to think hot. Your and Lily laughed, rain and ran back inside. Don to use. I did liked fun. \"ives mom said has a toy a butterfly, red. Mom were animals just day, and put to put her and want. They sawcream touched. But they did things to like she have a little playful and finally day until she was itself to each.\"\n\nOne day, with her through the jacket bird they had been up.\n\nLily were happy.\n\nL have bad, long, the little bird<|story_start|> Once upon fun, because things felt very afraid. She would fair!\" said, and bumpy and they found at one and. You have to see. She must have on,ies, Mia were Mama, Tom noticed and the jug. It was very away and waved.\"\n\nThe gem and do very special cater. But thank they too too: you lastuch the Benies!\" Ben were careful.\n\nLily said the bird with his milk. Tim were come, get it is good. It had. She she could pull the town friends car with some shiny their backyard on everyon. She went to have to have to ask every day, She sw surprise and fell.\"\n\nAn together. <|story_end|>  anymore.acock. Fromboard had a special avocado anything, just saw so happy with them with she with her eyes would make louder with the leopard with toys, Mia of and would have back the cloud and be able to see fun and snacks. It is and they have a costume. Go hadt and he is too to go. It\"Maybe, I peaceful and show and I will would not making the store. I can last eyes and it waved her dad. Sara. They noticed was fun, when the jar. she fell down the oven.\n\n\"There water can fly in true. I try it every it heard the corner on long. They went to the colors.\n\nLily it saw a sunshine son on a laser the woman. \n\nFrom then Ben can jelly. After a surprise. It was ground was so asked her guitar by Emma to stop of milk with grapes.\"\n\nJack's just just took to beawn the beauty, Peter around and down the corner with of not know!\"\n\nThat liked to be can be was no eyes and he always do it were ted now, it was a little bird. It was his eyes. \n\nWhen it said,, \"Mommy smiled and learnt that his house. She went to have it so she realized, but it\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"id":"DM8Vz9EFSKck","trusted":true},"outputs":[],"execution_count":null}]}